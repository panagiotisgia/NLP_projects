{"cells":[{"cell_type":"markdown","metadata":{"id":"fmBS2OanYmao"},"source":["# N-gram Languages Models\n","### 1st Assignment\n","\n","\n","* Veloudi Anthi, P3352101\n","* Giannopoulos Panagiotis, P3352102\n","* Lazaridou Styliani, P3352109 \n","* Orfanoudakis Christos, P3352113"]},{"cell_type":"markdown","metadata":{"id":"S39Ae7O9Yy06"},"source":["NLTK installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udcLFpRxYYuJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652889566922,"user_tz":-180,"elapsed":8918,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}},"outputId":"4ef122ff-de1d-45a7-855e-c3c45e4b44e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Collecting nltk\n","  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n","Collecting regex>=2021.8.3\n","  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n","\u001b[K     |████████████████████████████████| 749 kB 36.6 MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Installing collected packages: regex, nltk\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.7 regex-2022.4.24\n"]}],"source":["!pip install -U nltk"]},{"cell_type":"markdown","source":["# Models: Bigram & Trigram"],"metadata":{"id":"LJaRJFWImfb0"}},{"cell_type":"markdown","source":["## Import Dataset"],"metadata":{"id":"MkViAMBRjUi9"}},{"cell_type":"code","source":["# From Project Gutenberg download Lewis Carroll's Alice's Adventures in Wonderland ebook\n","\n","!curl https://www.gutenberg.org/files/11/11-0.txt --output alice.txt\n","\n","filename = \"alice.txt\"\n","f = open(str(filename), 'r')\n","\n","corpus = f.read().replace('*',' ') # Remove the '*' symbol from the corpus \n","f.close()"],"metadata":{"id":"X1nisKrfjZod","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652889567293,"user_tz":-180,"elapsed":382,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}},"outputId":"507242b7-ce64-4a0a-a3ec-7a6d48312bd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  170k  100  170k    0     0   500k      0 --:--:-- --:--:-- --:--:--  500k\n"]}]},{"cell_type":"markdown","metadata":{"id":"VuDs6EwLaF24"},"source":["## Sentence splitting\n","\n","In order to perform a tokenization, the particular NLTK tokenizer requires the `Punkt` sentence tokenization models to be installed."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HoeNB_NmaLGj","outputId":"9ab71dee-fcec-4d81-8831-d0687e28d567","executionInfo":{"status":"ok","timestamp":1652889569691,"user_tz":-180,"elapsed":2399,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","from nltk import sent_tokenize\n","\n","#Sentences tokenizer\n","alice_sents = sent_tokenize(corpus)"]},{"cell_type":"markdown","source":["## Test - Train split dataset (alice_sents)\n","We split our sentences in training and test part."],"metadata":{"id":"fIPrhpyvxe_W"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","alice_sents_train, alice_sents_test  = train_test_split(alice_sents, test_size=0.25, random_state=42)\n","\n","print(\"Sentences for train: \", len(alice_sents_train))\n","print(\"Sentences for test:  \", len(alice_sents_test))"],"metadata":{"id":"khy4lMkA0c-B","executionInfo":{"status":"ok","timestamp":1652889569692,"user_tz":-180,"elapsed":6,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"57abfa8f-8206-4aab-c1e1-d0b6b0b191e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentences for train:  826\n","Sentences for test:   276\n"]}]},{"cell_type":"markdown","source":["## Tokenization (word level)\n","Perform the tokenization in word level for the training sentences dataset. We use three different tokenizers."],"metadata":{"id":"bTCcNEIr-JFW"}},{"cell_type":"code","source":["import numpy as np\n","import itertools\n","from nltk import word_tokenize\n","from nltk import WhitespaceTokenizer\n","from nltk.tokenize import TweetTokenizer\n","\n","whitespace_wt = WhitespaceTokenizer()\n","tweet_wt = TweetTokenizer()\n","\n","words_1 = []\n","words_2 = []\n","words_3 = []\n","\n","# 1st Word Tokenizer\n","for sent in alice_sents_train:\n","    sent_tok = word_tokenize(sent)\n","    words_1.append(sent_tok)\n","\n","# 2nd Word Tokenizer\n","for sent in alice_sents_train:\n","    sent_tok = whitespace_wt.tokenize(sent)\n","    words_2.append(sent_tok)\n","\n","# 3rd Word Tokenizer\n","for sent in alice_sents_train:\n","    sent_tok = tweet_wt.tokenize(sent)\n","    words_3.append(sent_tok)   \n","\n","# Create list of words for the Train dataset\n","alice_words_1 = list(itertools.chain.from_iterable(words_1))\n","alice_words_2 = list(itertools.chain.from_iterable(words_2))\n","alice_words_3 = list(itertools.chain.from_iterable(words_3))\n","\n","print(\"1st tokenizer / Number of words: \", len(alice_words_1))\n","print(\"2nd tokenizer / Number of words: \", len(alice_words_2))\n","print(\"3rd tokenizer / Number of words: \", len(alice_words_3))"],"metadata":{"id":"HdVFpGHC1WXS","executionInfo":{"status":"ok","timestamp":1652889569946,"user_tz":-180,"elapsed":258,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a962b93-670e-404e-c5dd-b742c5c78a90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1st tokenizer / Number of words:  28206\n","2nd tokenizer / Number of words:  21903\n","3rd tokenizer / Number of words:  28761\n"]}]},{"cell_type":"markdown","metadata":{"id":"XXP8S3j9fMvc"},"source":["## Tokens frequency\n","Count the tokens frequency of each tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owXBcPPjfMoD","outputId":"654be312-44c6-47ad-821e-66d6f97b4dd8","executionInfo":{"status":"ok","timestamp":1652889569947,"user_tz":-180,"elapsed":7,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{} 1st_tokenizer most common tokens:\n","[(',', 1913),\n"," ('the', 1242),\n"," ('“', 831),\n"," ('”', 829),\n"," ('.', 683),\n"," ('and', 626),\n"," ('to', 588),\n"," ('’', 532),\n"," ('a', 504),\n"," ('of', 453)]\n","\n","\n","{} 2nd tokenizer most common tokens:\n","[('the', 1234),\n"," ('to', 579),\n"," ('and', 578),\n"," ('a', 500),\n"," ('of', 450),\n"," ('she', 352),\n"," ('in', 305),\n"," ('said', 303),\n"," ('it', 271),\n"," ('you', 231)]\n","\n","\n","{} 3rd tokenizer most common tokens:\n","[(',', 1913),\n"," ('the', 1247),\n"," ('.', 894),\n"," ('“', 831),\n"," ('”', 829),\n"," ('and', 647),\n"," ('to', 591),\n"," ('’', 532),\n"," ('a', 506),\n"," ('of', 455)]\n","\n","\n"]}],"source":["from pprint import pprint\n","\n","# 1st Tokenizer / Frequency distribution\n","count_1 = nltk.FreqDist(alice_words_1)\n","print('{} 1st_tokenizer most common tokens:')\n","pprint(count_1.most_common(10))\n","print('\\n')\n","\n","# 2nd Tokenizer / Frequency distribution\n","count_2 = nltk.FreqDist(alice_words_2)\n","print('{} 2nd tokenizer most common tokens:')\n","pprint(count_2.most_common(10))\n","print('\\n')\n","\n","# 3rd Tokenizer / Frequency distribution\n","count_3 = nltk.FreqDist(alice_words_3)\n","print('{} 3rd tokenizer most common tokens:')\n","pprint(count_3.most_common(10))\n","print('\\n')"]},{"cell_type":"markdown","source":["## Create Vocabulary and exclude unknown words\n","We create a vocabulary for each tokenizer. From this vocabulary we exlude the tokens (words) that appears less than one times. In the meantime we replace the above tokens with the '* ***UNK*** *' special character"],"metadata":{"id":"tzlnz5rZMxYY"}},{"cell_type":"code","source":["# Vocabulary / most and less freaqueny list _ count\n","\n","exlude_1 = list(filter(lambda x: x[1]<=1 , count_1.items()))\n","vocab_1 = list(filter(lambda x: x[1]>1 , count_1.items()))\n","print('1st tokenizer vocabulary')\n","print(\"Words exlude: \", len(exlude_1))\n","print(\"Words in vocabulary: \", len(vocab_1))\n","print('\\n')\n","\n","exlude_2 = list(filter(lambda x: x[1]<=1 , count_2.items()))\n","vocab_2 = list(filter(lambda x: x[1]>1 , count_2.items()))\n","print('2nd tokenizer vocabulary')\n","print(\"Words exlude: \", len(exlude_2))\n","print(\"Words in vocabulary: \", len(vocab_2))\n","print('\\n')\n","\n","exlude_3 = list(filter(lambda x: x[1]<=1 , count_3.items()))\n","vocab_3 = list(filter(lambda x: x[1]>1 , count_3.items()))\n","print('3rd tokenizer vocabulary')\n","print(\"Words exlude: \", len(exlude_3))\n","print(\"Words in vocabulary: \", len(vocab_3))\n","print('\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nxSgncpzQIjg","executionInfo":{"status":"ok","timestamp":1652889569947,"user_tz":-180,"elapsed":5,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}},"outputId":"86324b69-04ec-4a3b-c176-1b792d22f8af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1st tokenizer vocabulary\n","Words exlude:  1713\n","Words in vocabulary:  1560\n","\n","\n","2nd tokenizer vocabulary\n","Words exlude:  3092\n","Words in vocabulary:  1812\n","\n","\n","3rd tokenizer vocabulary\n","Words exlude:  1483\n","Words in vocabulary:  1574\n","\n","\n"]}]},{"cell_type":"code","source":["#Creation of Vocabulary and set exclude words as Unknown \n","\n","##### 1st tokenizer #####\n","vocabulary_1 = []\n","for i in range(len(vocab_1)):\n","    vocabulary_1 += [vocab_1[i][0]]\n","\n","for i in range(len(alice_sents_train)):\n","    words_1[i] = [x if x in vocabulary_1 else '*UNK*' for x in words_1[i]]\n","\n","##### 2nd tokenizer #####\n","vocabulary_2 = []\n","for i in range(len(vocab_2)):\n","    vocabulary_2 += [vocab_2[i][0]]\n","\n","for i in range(len(alice_sents_train)):\n","    words_2[i] = [x if x in vocabulary_2 else '*UNK*' for x in words_2[i]]\n","\n","##### 3rd tokenizer #####\n","vocabulary_3 = []\n","for i in range(len(vocab_3)):\n","    vocabulary_3 += [vocab_3[i][0]]\n","\n","for i in range(len(alice_sents_train)):\n","    words_3[i] = [x if x in vocabulary_3 else '*UNK*' for x in words_3[i]]"],"metadata":{"id":"jDoY1myUsG_8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LzYCcacqe2fZ"},"source":["## Create and count n-grams frequency (NLTK)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCLWHm7de3KA","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1652889570972,"user_tz":-180,"elapsed":8,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}},"outputId":"dae6c520-9660-4ef1-a3c4-816b97bf63ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nprint(\"============\")\\npprint(unigram_counter.most_common(10))\\nprint(\"============\")   \\npprint(bigram_counter.most_common(10))\\nprint(\"============\")\\npprint(trigram_counter.most_common(10))  \\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["from collections import Counter\n","from nltk.util import ngrams\n","\n","##### 1st tokenizer #####\n","unigram_counter_1 = Counter()\n","bigram_counter_1 = Counter()\n","trigram_counter_1 = Counter()\n","\n","for sent in words_1:\n","    # ngrams method does not add left and right pad symbols in unigrams, even if pad_left pad_right are True\n","    unigram_counter_1.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n","                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n","    bigram_counter_1.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n","                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n","    trigram_counter_1.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n","                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n","    \n","##### 2nd tokenizer #####\n","unigram_counter_2 = Counter()\n","bigram_counter_2 = Counter()\n","trigram_counter_2 = Counter()\n","\n","for sent in words_2:\n","    # ngrams method does not add left and right pad symbols in unigrams, even if pad_left pad_right are True\n","    unigram_counter_2.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n","                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n","    bigram_counter_2.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n","                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n","    trigram_counter_2.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n","                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n","\n","##### 3rd tokenizer #####\n","unigram_counter_3 = Counter()\n","bigram_counter_3 = Counter()\n","trigram_counter_3 = Counter()\n","\n","for sent in words_3:\n","    # ngrams method does not add left and right pad symbols in unigrams, even if pad_left pad_right are True\n","    unigram_counter_3.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n","                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n","    bigram_counter_3.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n","                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n","    trigram_counter_3.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n","                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n","\n","\"\"\"\n","print(\"============\")\n","pprint(unigram_counter.most_common(10))\n","print(\"============\")   \n","pprint(bigram_counter.most_common(10))\n","print(\"============\")\n","pprint(trigram_counter.most_common(10))  \n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"MIQn8SUNu9PC"},"source":["##  Calculate bigram probability\n","\n","### $ P(w_2|w_1) = \\frac{C(w_1,w_2) + \\alpha}{C(w_1) + \\alpha \\cdot|V|} $\n","\n","* $ C(w_1,w_2) $ : bigram count\n","* $ C(w_1) $ : unigram count\n","* $ 0 \\leq\\alpha \\leq1 $ :  smoothing hyper-parameter \n","* |V|: vocabulary size"]},{"cell_type":"code","source":["import math\n","\n","#### Sum of all bigrams/trigrams probabilities with Laplace smoothing ####\n","\n","# We should fine-tune alpha on a held-out dataset\n","alpha = 0.1  \n","\n","##### 1st tokenizer #####\n","sum_big_1 = 0\n","for e in bigram_counter_1:\n","  sum_big_1 += math.log2((bigram_counter_1[e] + alpha) / (unigram_counter_1[(e[0],)] + alpha*len(vocabulary_1)))\n","\n","sum_trig_1 = 0\n","for e in trigram_counter_1:\n","  sum_trig_1 += math.log2((trigram_counter_1[e] + alpha) / (bigram_counter_1[(e[0],e[1],)] + alpha*len(vocabulary_1)))\n","\n","##### 2nd tokenizer #####\n","sum_big_2 = 0\n","for e in bigram_counter_2:\n","  sum_big_2 += math.log2((bigram_counter_2[e] + alpha) / (unigram_counter_2[(e[0],)] + alpha*len(vocabulary_2)))\n","\n","sum_trig_2 = 0\n","for e in trigram_counter_2:\n","  sum_trig_2 += math.log2((trigram_counter_2[e] + alpha) / (bigram_counter_2[(e[0],e[1],)] + alpha*len(vocabulary_2)))\n","\n","##### 3rd tokenizer #####\n","sum_big_3 = 0\n","for e in bigram_counter_3:\n","  sum_big_3 += math.log2((bigram_counter_3[e] + alpha) / (unigram_counter_3[(e[0],)] + alpha*len(vocabulary_3)))\n","\n","sum_trig_3 = 0\n","for e in trigram_counter_3:\n","  sum_trig_3 += math.log2((trigram_counter_3[e] + alpha) / (bigram_counter_3[(e[0],e[1],)] + alpha*len(vocabulary_3)))\n","\n","print('1st tokenizer')\n","print(\"Sum of bigram probabilities:   {0:.3f}\".format(sum_big_1))\n","print(\"Sum of trigram probabilities:  {0:.3f}\".format(sum_trig_1))\n","print(\"============\")\n","print('2nd tokenizer')\n","print(\"Sum of bigram probabilities:   {0:.3f}\".format(sum_big_2))\n","print(\"Sum of trigram probabilities:  {0:.3f}\".format(sum_trig_2))\n","print(\"============\")\n","print('3rd tokenizer')\n","print(\"Sum of bigram probabilities:   {0:.3f}\".format(sum_big_3))\n","print(\"Sum of trigram probabilities:  {0:.3f}\".format(sum_trig_3))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1Dl3T6hxy5c","executionInfo":{"status":"ok","timestamp":1652889571499,"user_tz":-180,"elapsed":531,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}},"outputId":"25c3a7f8-8323-4c4c-e366-30baf0f101e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1st tokenizer\n","Sum of bigram probabilities:   -81773.290\n","Sum of trigram probabilities:  -141223.012\n","============\n","2nd tokenizer\n","Sum of bigram probabilities:   -79850.136\n","Sum of trigram probabilities:  -129440.160\n","============\n","3rd tokenizer\n","Sum of bigram probabilities:   -84142.613\n","Sum of trigram probabilities:  -144351.536\n"]}]},{"cell_type":"markdown","source":["# Models Evaluation: Cross-Entropy and Perplexity\n","To evaluate our models we will use the Cross-Entropy and Perplexity criterion. We will construct 6 models in total. A bigram model for each one of the three word tokenizers and a trigram model also for each one of the above tokenizers."],"metadata":{"id":"r4CBYOpAmmCJ"}},{"cell_type":"markdown","metadata":{"id":"AdGQ6lldvGDf"},"source":["## Bigram LM Cross entropy & perplexity\n","\n","* $ CrossEntropy = -\\frac{1}{N}\\sum^{bigrams}{log_2(P(w_2|w_1))} $\n","* N: Number of bigrams\n","* $ Perplexity = 2^{H(p)} $"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXP6NM36vF8n","outputId":"5b874baf-f0c6-43c1-97a8-e8d86b7c399f","executionInfo":{"status":"ok","timestamp":1652889571500,"user_tz":-180,"elapsed":6,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1st tokenizer\n","Cross Entropy: 5.682\n","Perplexity:    51.342\n","============\n","2nd tokenizer\n","Cross Entropy: 6.311\n","Perplexity:    79.386\n","============\n","3rd tokenizer\n","Cross Entropy: 5.713\n","Perplexity:    52.444\n"]}],"source":["alpha = 0.1\n","\n","##### 1st tokenizer #####\n","sum_prob_1 = 0\n","bigram_cnt_1 = 0\n","\n","for sent in words_1:\n","    sent = ['<s>'] + sent + ['<e>']\n","    # Iterate over the bigrams of the sentence\n","    for idx in range(1, len(sent)):\n","        bigram_prob_1 = (bigram_counter_1[(sent[idx-1], sent[idx])] + alpha) / (unigram_counter_1[(sent[idx-1],)] + alpha*len(vocabulary_1))\n","        sum_prob_1 += math.log2(bigram_prob_1)\n","        bigram_cnt_1 += 1\n","HC_1 = -sum_prob_1 / bigram_cnt_1\n","perpl_1 = math.pow(2,HC_1)\n","\n","##### 2nd tokenizer #####\n","sum_prob_2 = 0\n","bigram_cnt_2 = 0\n","\n","for sent in words_2:\n","    sent = ['<s>'] + sent + ['<e>']\n","    # Iterate over the bigrams of the sentence\n","    for idx in range(1, len(sent)):\n","        bigram_prob_2 = (bigram_counter_2[(sent[idx-1], sent[idx])] + alpha) / (unigram_counter_2[(sent[idx-1],)] + alpha*len(vocabulary_2))\n","        sum_prob_2 += math.log2(bigram_prob_2)\n","        bigram_cnt_2 += 1\n","HC_2 = -sum_prob_2 / bigram_cnt_2\n","perpl_2 = math.pow(2,HC_2)\n","\n","##### 3rd tokenizer #####\n","sum_prob_3 = 0\n","bigram_cnt_3 = 0\n","\n","for sent in words_3:\n","    sent = ['<s>'] + sent + ['<e>']\n","    # Iterate over the bigrams of the sentence\n","    for idx in range(1, len(sent)):\n","        bigram_prob_3 = (bigram_counter_3[(sent[idx-1], sent[idx])] + alpha) / (unigram_counter_3[(sent[idx-1],)] + alpha*len(vocabulary_3))\n","        sum_prob_3 += math.log2(bigram_prob_3)\n","        bigram_cnt_3 += 1\n","HC_3 = -sum_prob_3 / bigram_cnt_3\n","perpl_3 = math.pow(2,HC_3)\n","\n","\n","print('1st tokenizer')\n","print(\"Cross Entropy: {0:.3f}\".format(HC_1))\n","print(\"Perplexity:    {0:.3f}\".format(perpl_1))\n","print(\"============\")\n","print('2nd tokenizer')\n","print(\"Cross Entropy: {0:.3f}\".format(HC_2))\n","print(\"Perplexity:    {0:.3f}\".format(perpl_2))\n","print(\"============\")\n","print('3rd tokenizer')\n","print(\"Cross Entropy: {0:.3f}\".format(HC_3))\n","print(\"Perplexity:    {0:.3f}\".format(perpl_3))"]},{"cell_type":"markdown","metadata":{"id":"_IgMto59vPcL"},"source":["## Trigram LM Cross entropy & perplexity\n","\n","### $ P(w_3|w_1,w_2) = \\frac{C(w_1,w_2,w_3) + \\alpha}{C(w_1,w_2) + \\alpha \\cdot |V|} $\n","\n","* $ C(w_1,w_2,w_3) $ : trigram count\n","* $ C(w_1,w_2) $ : bigram count\n","* $ 0 \\leq\\alpha \\leq1 $ :  smoothing hyper-parameter \n","* |V|: vocabulary size"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNK7CX1tvPVl","outputId":"f502b98a-8e95-45be-f519-b7c782a5b15e","executionInfo":{"status":"ok","timestamp":1652889571749,"user_tz":-180,"elapsed":252,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1st tokenizer\n","Cross Entropy: 6.286\n","Perplexity:    78.044\n","============\n","2nd tokenizer\n","Cross Entropy: 6.815\n","Perplexity:    112.560\n","============\n","3rd tokenizer\n","Cross Entropy: 6.302\n","Perplexity:    78.895\n"]}],"source":["##### 1st tokenizer #####\n","sum_prob_1 = 0\n","trigram_cnt_1 = 0\n","\n","for sent in words_1:\n","    sent = ['<s>'] + ['<s>'] + sent + ['<e>'] + ['<e>']\n","    # Iterate over the trigrams of the sentence\n","    for idx in range(2,len(sent) - 1):\n","        trigram_prob_1 = (trigram_counter_1[(sent[idx-2],sent[idx-1], sent[idx])] + alpha) / (bigram_counter_1[(sent[idx-2],sent[idx-1])] + alpha*len(vocabulary_1))\n","        sum_prob_1 += math.log2(trigram_prob_1)\n","        trigram_cnt_1 += 1\n","HC_1 = -sum_prob_1 / trigram_cnt_1\n","perpl_1 = math.pow(2,HC_1)\n","\n","##### 2nd tokenizer #####\n","sum_prob_2 = 0\n","trigram_cnt_2 = 0\n","\n","for sent in words_2:\n","    sent = ['<s>'] + ['<s>'] + sent + ['<e>'] + ['<e>']\n","    # Iterate over the trigrams of the sentence\n","    for idx in range(2,len(sent) - 1):\n","        trigram_prob_2 = (trigram_counter_2[(sent[idx-2],sent[idx-1], sent[idx])] + alpha) / (bigram_counter_2[(sent[idx-2],sent[idx-1])] + alpha*len(vocabulary_2))\n","        sum_prob_2 += math.log2(trigram_prob_2)\n","        trigram_cnt_2 += 1\n","HC_2 = -sum_prob_2 / trigram_cnt_2\n","perpl_2 = math.pow(2,HC_2)\n","\n","##### 3rd tokenizer #####\n","sum_prob_3 = 0\n","trigram_cnt_3 = 0\n","\n","for sent in words_3:\n","    sent = ['<s>'] + ['<s>'] + sent + ['<e>'] + ['<e>']\n","    # Iterate over the trigrams of the sentence\n","    for idx in range(2,len(sent) - 1):\n","        trigram_prob_3 = (trigram_counter_3[(sent[idx-2],sent[idx-1], sent[idx])] + alpha) / (bigram_counter_3[(sent[idx-2],sent[idx-1])] + alpha*len(vocabulary_3))\n","        sum_prob_3 += math.log2(trigram_prob_3)\n","        trigram_cnt_3 += 1\n","HC_3 = -sum_prob_3 / trigram_cnt_3\n","perpl_3 = math.pow(2,HC_3)\n","\n","\n","print('1st tokenizer')\n","print(\"Cross Entropy: {0:.3f}\".format(HC_1))\n","print(\"Perplexity:    {0:.3f}\".format(perpl_1))\n","print(\"============\")\n","print('2nd tokenizer')\n","print(\"Cross Entropy: {0:.3f}\".format(HC_2))\n","print(\"Perplexity:    {0:.3f}\".format(perpl_2))\n","print(\"============\")\n","print('3rd tokenizer')\n","print(\"Cross Entropy: {0:.3f}\".format(HC_3))\n","print(\"Perplexity:    {0:.3f}\".format(perpl_3))"]},{"cell_type":"markdown","source":["# Context-aware spelling corrector\n","We will create a context-aware spelling corrector. At first we calculate the edit distance of each word and return the 5 nearest to them according to our vocabulary. Then acccording to our bigram model we will calculate the correct syntax."],"metadata":{"id":"XADAw7K0soMx"}},{"cell_type":"code","source":["#Function to calculate the edit distance and return the 5 nearest words.\n","def ED_closer(word):\n","  dist = []\n","  result = []\n","  K = 5\n","\n","  for voc in vocabulary_3:\n","    dist += [nltk.edit_distance(voc , word )]\n","\n","  res = sorted(range(len(dist)), key = lambda sub: dist[sub])[:K]\n","  for i in range(K):  \n","    result += [vocabulary_3[res[i]]]\n","\n","  return result"],"metadata":{"id":"F-7MoU2J8gLe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Test for word already in the text\n","ED_closer('Alice')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-DJbYXVCBY-","executionInfo":{"status":"ok","timestamp":1652889571750,"user_tz":-180,"elapsed":4,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}},"outputId":"6617314a-98d1-44c6-a807-30a4a7a417a3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Alice', 'like', 'voice', 'line', 'twice']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["#Test for missplening word in the text\n","ED_closer('notinhg')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TW3Sfw1WCFv-","executionInfo":{"status":"ok","timestamp":1652889572068,"user_tz":-180,"elapsed":320,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}},"outputId":"997d4cea-49df-4e2d-b4d6-7c8d3a332452"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['nothing', 'coming', 'notion', 'going', 'notice']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["#Function for spelling and context correction\n","\n","def spelling_context_corrector(wrong):\n","  #Use the tweet tokenizer to split the sentences\n","  wrong_tok = tweet_wt.tokenize(wrong)\n","  right = ['<s>']\n","\n","  for ind_words in range(len(wrong_tok)):\n","    #Calculation the 5 nearest words\n","    pred = ED_closer(wrong_tok[ind_words])\n","    prob = []\n","    \n","    for idx in range(len(pred)):\n","      #Bigrams creation and probability calculation\n","      e = (right[ind_words] , pred[idx])\n","      prob += [math.log2((bigram_counter_3[e] + alpha) / (unigram_counter_3[(e[0],)] + alpha*len(vocabulary_3)))]\n","    \n","    #Returns bigrams with the highest probabilities\n","    max_value = max(prob)\n","    right += [pred[prob.index(max_value)]]\n","\n","  right_text = ' '.join(right[1:])\n","\n","  print('Input     = ', wrong)\n","  print('Corrected = ', right_text)\n","\n","  return right_text"],"metadata":{"id":"Dn8leuOuA_v6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing Corrector\n","We will test the above corrector. We will give as input a sentence with two types of errors."],"metadata":{"id":"EZnJb0RRmzZ4"}},{"cell_type":"code","source":["wrong = 'Either te wel was very dee, or she fell vedry slowly, for she had \\\n","plenty of time as she went duwn to like about he and to wander wet \\\n","was goin to hupen nixt.'"],"metadata":{"id":"fdJJODdtApbM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["right = spelling_context_corrector(wrong)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_S8pwwkQK13","executionInfo":{"status":"ok","timestamp":1652889574759,"user_tz":-180,"elapsed":2692,"user":{"displayName":"Στέλλα Λαζαρίδου","userId":"11526350240926337706"}},"outputId":"8a6e5808-d5d5-4a81-a981-8553b47bd34a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input     =  Either te wel was very dee, or she fell vedry slowly, for she had plenty of time as she went duwn to like about he and to wander wet was goin to hupen nixt.\n","Corrected =  either the well as very deep , for the well very slowly , for the had left off the as she went down to lie about the end to wonder wet as going to happen next .\n"]}]},{"cell_type":"markdown","source":["## Conclusion\n","As we can see our corrector have a good performance in misspelling words but it can't give good context aware recommendations. That is primary due to small size of our vocabulary and the poor estimations of the bigram model."],"metadata":{"id":"vyzeZDbMyVpe"}},{"cell_type":"markdown","metadata":{"id":"KjyYBcFWvtMC"},"source":["#Resources\n","* https://www.nltk.org\n","* https://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk\n","* https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize\n","* https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger   \n","* https://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization"]}],"metadata":{"colab":{"name":"1st_Assignment_Notebook.ipynb","provenance":[{"file_id":"1m5YaD4CABNhdkfQbG5SGhq8BHgMFZZUd","timestamp":1650116132138},{"file_id":"1y2F_XyZtX3ySKWggxESs_6mBxNtrN2JL","timestamp":1649352361193}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}